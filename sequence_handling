#!/bin/bash

set -o pipefail

function Usage() {
    echo -e "\
Usage:  `basename $0` <handler> <config> [-t custom_array_indices]
Where:  <handler> is one of:
            Recommended workflow:
                1 | Quality_Assessment
                2 | Adapter_Trimming
                3 | Read_Mapping
                4 | SAM_Processing
                5 | Coverage_Mapping
                6 | Haplotype_Caller
                7 | Genomics_DB_Import
                8 | Genotype_GVCFs
                9 | Create_HC_Subset
                10 | Variant_Recalibrator
                11 | Variant_Filtering
                12 | Variant_Analysis
            Other handlers:
                13 | GBS_Demultiplex (in progress)
                14 | Quality_Trimming
                15 | Realigner_Target_Creator
                16 | Indel_Realigner
                17 | Freebayes_Variant_Calling
            Nanopore workflow:
                1NP | NP_Quality_Assessment (in progress)
                2NP | NP_Adapter_Trimming (in progress)
                3NP | NP_Read_Mapping (in progress)
                4NP | NP_SAM_Processing (in progress)
And:    <config> is the full file path to the configuration file

Optional arguments:
        [-t custom_array_indices] is a range of arrays and/or comma separated list of specific arrays to run.
        Important: No spaces allowed when providing range or list of arrays.
            and -t flag must be provided as the 3rd argument on the command line (see example below)
        If left blank, the DEFAULT runs all samples.
        This is helpful if only some of your jobs arrays fail and you need to re-run only those.

        Example: ./sequence_handling SAM_Processing /path/to/config -t 1-5,10,12
" >&2
    exit 1
}

export -f Usage

#   Where is 'sequence_handling' located?
#   Mod by Naoki
SCRIPT=$(realpath $0)  # Track back the symlink to find the actual location
SEQUENCE_HANDLING=$(dirname "${SCRIPT}")

#   A list of valid sequencing platforms
VALID_SEQ_PLATFORMS=('CAPILLARY' 'LS454' 'ILLUMINA' 'SOLID' 'HELICOS' 'IONTORRENT' 'ONT' 'PACBIO')

#   If we have less than two arguments
if [[ "$#" -lt 1 ]]; then Usage; fi # Display the usage message and exit

ROUTINE="$1" # What routine are we running?
CONFIG="$2" # Where is our config file?

#   If the specified config exists
if [[ -f "${CONFIG}" ]]
then
    source "${CONFIG}" # Source it, providing parameters and software
    bash "${CONFIG}" > /dev/null 2> /dev/null # Load any modules
    source "${SEQUENCE_HANDLING}"/HelperScripts/utils.sh # And the utils script
else # If it doesn't
    echo "Please specify a valid config file." >&2 # Print error message
    exit 1 # Exit with non-zero exit status
fi

#   After loading Config, make sure that we will be able to properly output files
mkdir -p "${OUT_DIR}"
if ! [[ -w "${OUT_DIR}" ]]; then echo "You don't have write permissions for the output directory ${OUT_DIR}, exiting..." >&2; exit 1; fi

#   Custom list of job arrays to run using a -t flag
#   Currently, -t flag has to be provided as 3rd argument
case "$3" in
    -t)
        echo "Using custom list of job arrays provided from -t flag on the command line..." >&2
        #   Do we have a list of custom job arrays to run?
        #   Acceptable formats include range separated by hyphen, comma separated list, or mix of both
        CUSTOM_JOB_ARR="$4"
    ;;
esac

#   Where do we output the standard error and standard output files?
ERROR="${OUT_DIR}"/Error_Files
mkdir -p "${ERROR}"

#   Run sequence_handling
case "${ROUTINE}" in
    1 | Quality_Assessment)
        echo "$(basename $0): Assessing quality..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Quality_Assessment.sh
        checkDependencies Quality_Assessment_Dependencies[@] # Check to see if dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${QA_SAMPLES}" # Check that the samples and sample list exist
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
	    if [[  "$USE_PBS" == "true" ]]; then
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Quality_Assessment.sh && Quality_Assessment ${QA_SAMPLES} ${OUT_DIR} ${PROJECT} ${TARGET}" | qsub -l "${QA_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Quality_Assessment
	    else
	        (source "${CONFIG}" && source "${SEQUENCE_HANDLING}/Handlers/Quality_Assessment.sh" && Quality_Assessment "${QA_SAMPLES}" "${OUT_DIR}" "${PROJECT}" "${TARGET}") > "${ERROR}/Quality_Assessment.log" 2>&1
	    fi
        ;;
    2 | Adapter_Trimming )
        echo "$(basename $0): Trimming adapters..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Adapter_Trimming.sh
        checkDependencies Adapter_Trimming_Dependencies[@] # Check to see if dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${RAW_SAMPLES}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        if ! [[ -f "${ADAPTERS}" ]]; then echo "Please specify a valid adapters file" >&2; exit 1; fi # Check for a valid adapters file
        if [[ -z "${QUAL_ENCODING}" ]]; then echo "Please specify the QUAL_ENCODING in the config file" >&2; exit 1; fi # Make sure the platform is filled out
	    if [[ "$USE_PBS" == "true" ]]; then
            #   Run Adapter_Trimming using a task array
            declare -a AT_LIST=($(grep -E ".fastq|.fastq.gz" "${RAW_SAMPLES}"))
            SINGLE_ARRAY_LIMIT=$[${#AT_LIST[@]} - 1] # Get the maximum number of Torque tasks we're doing for our samples
            echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
            if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]
            then # If we have enough samples for a task array
                # If CUSTOM_JOB_ARR is NOT set (only gets set if user uses -t flag)
                if [[ -z ${CUSTOM_JOB_ARR+x} ]]
                then
                    echo "Using default array limit from 0 to ${SINGLE_ARRAY_LIMIT}"
		            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Adapter_Trimming.sh && Adapter_Trimming ${RAW_SAMPLES} ${OUT_DIR} ${PROJECT} ${FORWARD_NAMING} ${REVERSE_NAMING} ${ADAPTERS} ${PRIOR} ${QUAL_ENCODING}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -l "${AT_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Adapter_Trimming
                else
                    # Use job arrays following -t flag
                    echo "Using custom job arrays following -t flag: ${CUSTOM_JOB_ARR}" >&2
                    echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Adapter_Trimming.sh && Adapter_Trimming ${RAW_SAMPLES} ${OUT_DIR} ${PROJECT} ${FORWARD_NAMING} ${REVERSE_NAMING} ${ADAPTERS} ${PRIOR} ${QUAL_ENCODING}" | qsub -t "${CUSTOM_JOB_ARR}" -l "${AT_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Adapter_Trimming
                fi
            else # If we have only one sample
		        echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Adapter_Trimming.sh && Adapter_Trimming ${RAW_SAMPLES} ${OUT_DIR} ${PROJECT} ${FORWARD_NAMING} ${REVERSE_NAMING} ${ADAPTERS} ${PRIOR} ${QUAL_ENCODING}" | qsub -t 0 -l "${AT_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Adapter_Trimming
            fi
	    else
	        #   Non PBS processing
	        (source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Adapter_Trimming.sh && Adapter_Trimming ${RAW_SAMPLES} ${OUT_DIR} ${PROJECT} ${FORWARD_NAMING} ${REVERSE_NAMING} ${ADAPTERS} ${PRIOR} ${QUAL_ENCODING}) > ${ERROR}/Adapter_Trimming.log 2>&1
	    fi
        ;;
    3 | Read_Mapping )
        echo "`basename $0`: Mapping reads...">&2
        source "${SEQUENCE_HANDLING}"/Handlers/Read_Mapping.sh
        checkDependencies Read_Mapping_Dependencies[@] # Check to see if dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        # Check samples
        if [[ -z ${CUSTOM_JOB_ARR+x} ]] # If CUSTOM_JOB_ARR is NOT set (only gets set if user uses -t flag)
        then
            # Check to see if all samples and sample list exists
            checkSamples "${TRIMMED_LIST}" # Check to see if samples and sample list exists
        else
            # Only check that samples in CUSTOM_JOB_ARR exists
            # Purpose: if we need to re-run some samples and do not want to re-download all the samples
            # (due to space limitation), this allows checking of only those samples to re-run without having
            # to change our sample list. The sample list can still include all the samples.
            echo "Using custom job arrays following -t flag. Only check if custom job array samples exist."
            checkSamplesCustomJobArrRM "${TRIMMED_LIST}" "${CUSTOM_JOB_ARR}" "${SINGLES_TRIMMED}" "${FORWARD_TRIMMED}" "${REVERSE_TRIMMED}"
        fi
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        checkIndex "${REF_GEN}" # Check to make sure our reference genome is indexed
        if [[ "$?" -ne 0 ]]; then echo "Reference genome is not indexed for BWA mem..." >&2; indexReference "${REF_GEN}"; fi # If not, index it and exit
        if [[ -z "${SEQ_PLATFORM}" ]]; then echo "Please specify the SEQ_PLATFORM in the config file" >&2; exit 1; fi # Make sure the platform is filled out
        [[ "${VALID_SEQ_PLATFORMS[@]}" =~ "${SEQ_PLATFORM}" ]] || (echo -e "'${SEQ_PLATFORM}' is not a valid platform\nPlease choose from:" >&2; for PLAT in ${VALID_SEQ_PLATFORMS[@]}; do echo -e "\t${PLAT}"; done; exit 1)
        declare -a SINGLE_SAMPLES=($(grep -E "${SINGLES_TRIMMED}" "${TRIMMED_LIST}")) # Get the single-end samples
        declare -a FORWARD_SAMPLES=($(grep -E "${FORWARD_TRIMMED}" "${TRIMMED_LIST}")) # Get the forward samples
        declare -a REVERSE_SAMPLES=($(grep -E "${REVERSE_TRIMMED}" "${TRIMMED_LIST}")) # Get the reverse samples
        if [[ ! -z "${FORWARD_SAMPLES[@]}" && ! -z "${REVERSE_SAMPLES[@]}" ]] # If we have paired-end samples
        then
            declare -a PAIRED_NAMES=($(parallel basename {} "${FORWARD_TRIMMED}" ::: "${FORWARD_SAMPLES[@]}")) # Create an array of paired-end sample names
        fi
	    if ! [[ -z "${SINGLE_SAMPLES[@]}" ]]
        then # If we have single-end samples
            MODE="single";
            SINGLE_ARRAY_LIMIT=$[${#SINGLE_SAMPLES[@]} - 1]
        elif [[ ! -z "${FORWARD_SAMPLES[@]}" && ! -z "${REVERSE_SAMPLES[@]}" ]]
        then # If we have paired-end samples
            MODE="paired";
            SINGLE_ARRAY_LIMIT=$[${#FORWARD_SAMPLES[@]} - 1]
            # Make sure we have equal numbers of forward and reverse samples
            if [[ "${#FORWARD_SAMPLES[@]}" -ne "${#REVERSE_SAMPLES[@]}" ]]; then echo "Unequal numbers of forward and reverse reads, exiting..." >&2; exit 1; fi
        else
            echo "ERROR: No samples match the provided suffix \"${FORWARD_TRIMMED}\", exiting..." >&2;
            exit 17;
        fi
	    if [[ "$USE_PBS" == "true" ]]; then
            echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
            if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]
            then # If we have enough samples for a task array
                # If CUSTOM_JOB_ARR is NOT set (only gets set if user uses -t flag)
                if [[ -z ${CUSTOM_JOB_ARR+x} ]]
                then
                    echo "Using default array limit from 0 to ${SINGLE_ARRAY_LIMIT}"
                    echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Read_Mapping.sh && Read_Mapping ${TRIMMED_LIST} ${SINGLES_TRIMMED} ${FORWARD_TRIMMED} ${REVERSE_TRIMMED} ${MODE} ${PROJECT} ${SEQ_PLATFORM} ${OUT_DIR} ${REF_GEN}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -l "${RM_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Read_Mapping
                else
                    # Use job arrays following -t flag
                    echo "Using custom job arrays following -t flag: ${CUSTOM_JOB_ARR}" >&2
                    echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Read_Mapping.sh && Read_Mapping ${TRIMMED_LIST} ${SINGLES_TRIMMED} ${FORWARD_TRIMMED} ${REVERSE_TRIMMED} ${MODE} ${PROJECT} ${SEQ_PLATFORM} ${OUT_DIR} ${REF_GEN}" | qsub -t "${CUSTOM_JOB_ARR}" -l "${RM_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Read_Mapping
                fi
            else # If we only have one sample
		        echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Read_Mapping.sh && Read_Mapping ${TRIMMED_LIST} ${SINGLES_TRIMMED} ${FORWARD_TRIMMED} ${REVERSE_TRIMMED} ${MODE} ${PROJECT} ${SEQ_PLATFORM} ${OUT_DIR} ${REF_GEN}" | qsub -t 0 -l "${RM_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Read_Mapping
            fi
	    else # without PBS
	        (source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Read_Mapping.sh && Read_Mapping ${TRIMMED_LIST} ${SINGLES_TRIMMED} ${FORWARD_TRIMMED} ${REVERSE_TRIMMED} ${MODE} ${PROJECT} ${SEQ_PLATFORM} ${OUT_DIR} ${REF_GEN}) > ${ERROR}/Read_Mapping.log 2>&1
	    fi
        ;;
    4 | SAM_Processing )
        case "${METHOD}" in
            samtools )
                echo "$(basename $0): Processing SAM files using SAMtools..." >&2
                source "${SEQUENCE_HANDLING}"/Handlers/SAM_Processing_SAMtools.sh
                checkDependencies SAM_Processing_Dependencies[@] # Check to see if the dependencies are installed
                if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
                checkSamples "${MAPPED_LIST}" # Check to see if samples and sample list exists
                if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
                checkFaidx "${REF_GEN}" # Check to see if reference genome is indexed
                if [[ "$?" -ne 0 ]]; then echo "Reference genome is not indexed for SAM Processing...">&2; fadixReference "${REF_GEN}"; fi # If not, index and exit
                checkVersion 'samtools' '1.3' # Check SAMtools version 1.3 or higher
                if [[ "$?" -ne 0 ]]; then echo "Please use SAMtools version 1.3 or higher" >&2; exit 1; fi
                #   Run SAM_Processing
		        if [[ ${USE_PBS} == true ]]; then
                    echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/SAM_Processing_SAMtools.sh && SAM_Processing ${MAPPED_LIST} ${OUT_DIR} ${REF_GEN} ${PROJECT}" | qsub -l "${SP_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_SAM_Processing
		        else
		            (source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/SAM_Processing_SAMtools.sh && SAM_Processing ${MAPPED_LIST} ${OUT_DIR} ${REF_GEN} ${PROJECT}) > ${ERROR}/SAM_Processing_SAMtools.log 2>&1
		        fi
                ;;
            picard )
                echo "$(basename $0): Processing SAM files using Picard..." >&2
                source "${SEQUENCE_HANDLING}"/Handlers/SAM_Processing_Picard.sh
                checkDependencies SAM_Processing_Dependencies[@] # Check to see if the dependencies are installed
                if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
                checkSamples "${MAPPED_LIST}" # Check to see if samples and sample list exists
                if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
                checkPicard "${PICARD_JAR}" # Check to make sure Picard is installed
                if [[ "$?" -ne 0 ]]; then exit 1; fi # If we don't have Picard, exit with error
                checkVersion 'samtools' '1.3' # Check SAMtools version 1.3 or higher
                if [[ "$?" -ne 0 ]]; then echo "Please use SAMtools version 1.3 or higher" >&2; exit 1; fi
                if [[ -z "${SEQ_PLATFORM}" ]]; then echo "Please specify the SEQ_PLATFORM in the config file" >&2; exit 1; fi # Make sure the platform is filled out
                [[ "${VALID_SEQ_PLATFORMS[@]}" =~ "${SEQ_PLATFORM}" ]] || (echo -e "'${SEQ_PLATFORM}' is not a valid platform\nPlease choose from:" >&2; for PLAT in ${VALID_SEQ_PLATFORMS[@]}; do echo -e "\t${PLAT}"; done; exit 1)
                SP_MEM=$(getMemory "${SP_QSUB}") # Figure out memory requirements based on the Qsub settings
                #   Create the header for the mapping stats summary file
                mkdir -p "${OUT_DIR}/SAM_Processing/Picard/Statistics"
                echo -e "Sample name\tTotal reads\tPercent mapped\tPercent paired\tPercent singletons\tFraction with mate mapped to different chr" > "${OUT_DIR}/SAM_Processing/Picard/Statistics/${PROJECT}_mapping_summary.tsv"
                #   Run SAM_Processing using a task array
                declare -a SAM_LIST=($(grep -E ".sam" "${MAPPED_LIST}"))
                SINGLE_ARRAY_LIMIT=$[${#SAM_LIST[@]} - 1] # Get the maximum number of Torque tasks we're doing for SAM samples
                echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
		        if [[ "${USE_PBS}" == "true" ]]
                then
                    echo -e "#!/bin/bash\n#PBS -l ${SP_QSUB}\n#PBS -e ${ERROR}\n#PBS -o ${ERROR}\n#PBS -m abe\n#PBS -M ${EMAIL}\nset -e\nset -o pipefail\nsource ${CONFIG}\nsource ${SEQUENCE_HANDLING}/Handlers/SAM_Processing_Picard.sh\ndeclare -a SAM_LIST=($(grep -E ".sam" "${MAPPED_LIST}"))\nSINGLE_ARRAY_LIMIT=\$[\${#SAM_LIST[@]} - 1]\nSAM_Processing \${SAM_LIST[\${PBS_ARRAYID}]} ${OUT_DIR} ${PICARD_JAR} ${SEQ_PLATFORM} ${SP_MEM} ${MAX_FILES} ${PROJECT} ${TMP}" > ${PROJECT}_SAM_Processing
                    if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]
                    then # If we have enough samples for a task array
                        # If CUSTOM_JOB_ARR is NOT set (only gets set if user uses -t flag)
                        if [[ -z ${CUSTOM_JOB_ARR+x} ]]
                        then
                            echo "Using default array limit from 0 to ${SINGLE_ARRAY_LIMIT}"
			                qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -q "${SP_QUEUE}" "${PROJECT}"_SAM_Processing
                        else
                            # Use job arrays following -t flag
                            echo "Using custom job arrays following -t flag: ${CUSTOM_JOB_ARR}" >&2
                            qsub -t "${CUSTOM_JOB_ARR}" -q "${SP_QUEUE}" "${PROJECT}"_SAM_Processing
                        fi
                    else # If we only have one sample
			            qsub -t 0 "${PROJECT}" -q "${SP_QUEUE}" _SAM_Processing
                    fi
                    #   Remove the file that created the task array
                    rm "${PROJECT}"_SAM_Processing
		        else   # NO PBS
		            if [[ -z "${SAM_PROCESSING_THREADS}" ]]; then
			            SAM_PROCESSING_THREADS=0   # Use all cores if not defined
		            fi
		            (set -eo pipefail; source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/SAM_Processing_Picard.sh && printf '%s\n' "${SAM_LIST[@]}" | parallel --jobs ${SAM_PROCESSING_THREADS} "SAM_Processing {} ${OUT_DIR} ${PICARD_JAR} ${SEQ_PLATFORM} ${SP_MEM} ${MAX_FILES}  ${PROJECT} ${TMP}") > ${ERROR}/SAM_Processing_Picard.log 2>&1
		        fi
                ;;
            * )
                echo "Invalid method"
                exit 1
                ;;
        esac
        ;;
    5 | Coverage_Mapping )
        echo "$(basename $0): Mapping coverage..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Coverage_Mapping.sh
        checkDependencies Coverage_Mapping_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkVersion bedtools 2.17.0 # Check to see that we have bedtools version 2.17.0 or newer
        if [[ "$?" -ne 0 ]]; then echo "Please use Bedtools version 2.17.0" >&2; exit 1; fi # If not, exit out with error
	    # With version 2.24.0 or newer, the behavior of bedtools coverage was changed.
        checkVersion bedtools 2.24.0 # Check to see that we have bedtools version 2.24.0
	    if [[ "$?" -eq 0 ]]; then
	        bedtoolsPre2_24_0="false"
	    else
	        bedtoolsPre2_24_0="true"
	    fi
        #checkVersion R 3.3 # Check to see that we have R version 3.3.X
        #if [[ "$?" -ne 0 ]]; then echo "Please use R version 3.3.X" >&2; exit 1; fi # If not, exit out with error
        checkSamples "${BAM_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        if ! [[ -f "${REGIONS_FILE}" ]]
            then echo "No regions file found, assuming whole-genome sequencing data..." >&2
            else echo "Using regions file at ${REGIONS_FILE} for exome capture data..." >&2
        fi
	    if [[  "$USE_PBS" == "true" ]]
        then
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Coverage_Mapping.sh && Coverage_Mapping ${BAM_LIST} ${OUT_DIR} ${PROJECT} ${bedtoolsPre2_24_0} ${REGIONS_FILE}" | qsub -l "${CM_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Coverage_Mapping
	    else
	        (source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Coverage_Mapping.sh && Coverage_Mapping ${BAM_LIST} ${OUT_DIR} ${PROJECT} ${bedtoolsPre2_24_0} ${REGIONS_FILE}) > ${ERROR}/Coverage_Mapping.log 2>&1
	    fi
        ;;
    6 | Haplotype_Caller )
        echo "$(basename $0): Calling SNPs with GATK Haplotype_Caller..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Haplotype_Caller.sh
        checkDependencies Haplotype_Caller_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${FINISHED_BAM_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        checkBaiIndex "${FINISHED_BAM_LIST}" # Check to see if our samples are indexed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If they're not indexed, exit out with error
	    # if GATK_JAR is not set in Config, it will check GATK_LOCAL_JAR
        GATK_JAR=$(checkGATK "${GATK_JAR}") # Check to make sure GATK is installed
        if [[ "${GATK_JAR}" == 1 ]]; then exit 1; fi # If we don't have GATK, exit with error
	    if checkVersion gatk 4.0 ; then   # check gatk version
	        gatkVer=4
	    elif checkVersion gatk 3.8.0; then
	        gatkVer=3
	    else
	        echo "Please install GATK 3.8.0 or newer" >&2
	        exit 1
	    fi
        if [[ -z "${DO_NOT_TRIM_ACTIVE_REGIONS}}" ]]; then echo "Please put 'true' or 'false' for DO_NOT_TRIM_ACTIVE_REGIONS in the config file" >&2; exit 1; fi # Make sure barley is filled out
        if [[ -z "${FORCE_ACTIVE}" ]]; then echo "Please put 'true' or 'false' for FORCE_ACTIVE in the config file" >&2; exit 1; fi # Make sure barley is filled out
        HC_MEM=$(getMemory "${HC_QSUB}") # Figure out memory requirements based on the Qsub settings
        HC_THREADS=$(getThreads "${HC_QSUB}") # Figure out the number of threads we have
        checkDict "${REF_GEN}" # Check to make sure our reference genome has a dict file
        if [[ "$?" -ne 0 ]]; then echo "Generating dictionary file for ${REF_GEN}..." >&2; createDict "${REF_GEN}" "${HC_MEM}" "${PICARD_JAR}"; fi # If not, generate one
        declare -a HC_LIST=($(grep -E ".bam" "${FINISHED_BAM_LIST}")) # Create an array of the BAM files
        # If we are parallelizing across custom regions for every sample, we will have
        # a total array limit of num_intervals * num_samples
        if [ "${HC_PARALLELIZE}" == "true" ] && [ "${HC_CUSTOM_INTERVALS}" != false ]; then
            # Generate list of samples and intervals
            new_arr=()
            for i in $(cat ${FINISHED_BAM_LIST})
            do
                for x in $(cat ${HC_CUSTOM_INTERVALS})
                do
                    temp=$(printf "${i}-${x}\n")
                    new_arr+=( ${temp} )
                done
            done
            # If we also have scaffolds in addition to custom intervals
            # and we are parallelizing across regions, add one to the array limit
            if [[ "${HC_SCAFFOLDS}" != "false" ]]; then
                for i in $(cat ${FINISHED_BAM_LIST})
                do
                    temp=$(printf "${i}-${HC_SCAFFOLDS}\n")
                    new_arr+=( ${temp} )
                done
            fi
            # Get the maximum number of Torque tasks
            SINGLE_ARRAY_LIMIT=$[${#new_arr[@]} - 1]
        else
            # Not parallelizing, total array limit is equal to the number of samples
            SINGLE_ARRAY_LIMIT=$[${#HC_LIST[@]} - 1] # Get the maximum number of Torque tasks (# in array - 1)
            # If we also have scaffolds, add one to the array limit
            if [[ "${HC_SCAFFOLDS}" != "false" ]]; then
                SINGLE_ARRAY_LIMIT=${#HC_LIST[@]}
            fi
        fi
        echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
	    if [[ "$USE_PBS" == "true" ]]; then
            if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]
            then # If we have enough samples for a task array
                # If CUSTOM_JOB_ARR is NOT set (only gets set if user uses -t flag)
                if [[ -z ${CUSTOM_JOB_ARR+x} ]]
                then
                    echo "Using default array limit from 0 to ${SINGLE_ARRAY_LIMIT}"
		            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Haplotype_Caller.sh && Haplotype_Caller ${FINISHED_BAM_LIST} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${THETA} ${HC_MEM} ${HC_THREADS} ${FIX_QUALITY_SCORES} ${DO_NOT_TRIM_ACTIVE_REGIONS} ${FORCE_ACTIVE} ${gatkVer} ${HC_PARALLELIZE} ${HC_CUSTOM_INTERVALS} ${HC_SCAFFOLDS} ${TMP}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -q "${HC_QUEUE}" -l "${HC_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Haplotype_Caller
                else
                    # Use job arrays following -t flag
                    echo "Using custom job arrays following -t flag: ${CUSTOM_JOB_ARR}" >&2
                    echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Haplotype_Caller.sh && Haplotype_Caller ${FINISHED_BAM_LIST} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${THETA} ${HC_MEM} ${HC_THREADS} ${FIX_QUALITY_SCORES} ${DO_NOT_TRIM_ACTIVE_REGIONS} ${FORCE_ACTIVE} ${gatkVer} ${HC_PARALLELIZE} ${HC_CUSTOM_INTERVALS} ${HC_SCAFFOLDS} ${TMP}" | qsub -t "${CUSTOM_JOB_ARR}" -q "${HC_QUEUE}" -l "${HC_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Haplotype_Caller
                fi
            else # If we have only one sample
		        echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Haplotype_Caller.sh && Haplotype_Caller ${FINISHED_BAM_LIST} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${THETA} ${HC_MEM} ${HC_THREADS} ${FIX_QUALITY_SCORES} ${DO_NOT_TRIM_ACTIVE_REGIONS} ${FORCE_ACTIVE} ${gatkVer} ${HC_PARALLELIZE} ${HC_CUSTOM_INTERVALS} ${HC_SCAFFOLDS} ${TMP}" | qsub -t 0 -q "${HC_QUEUE}" -l "${HC_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Haplotype_Caller
            fi
	    else # without PBS
	        (source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Haplotype_Caller.sh && Haplotype_Caller ${FINISHED_BAM_LIST} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${THETA} ${HC_MEM} ${HC_THREADS} ${FIX_QUALITY_SCORES} ${DO_NOT_TRIM_ACTIVE_REGIONS} ${FORCE_ACTIVE} ${gatkVer} ${HC_PARALLELIZE} ${HC_CUSTOM_INTERVALS} ${HC_SCAFFOLDS} ${TMP}) > ${ERROR}/Haplotype_Caller.log 2>&1
	    fi
        ;;
    7 | Genomics_DB_Import )
        echo "$(basename $0): Genomics DB Import..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Genomics_DB_Import.sh
        checkDependencies Genomics_DB_Import_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${GVCF_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        checkGvcfIndex "${GVCF_LIST}" "${OUT_DIR}" # Check if .g.vcf files are indexed
	    # if GATK_JAR is not set in Config, it will check GATK_LOCAL_JAR
	    GATK_JAR=$(checkGATK "${GATK_JAR}") # Check to make sure GATK is installed
        if [[ "${GATK_JAR}" == 1 ]]; then exit 1; fi # If we don't have GATK, exit with error
	    rm -f "${ERROR}/Genotype_GVCFs.log"; touch "${ERROR}/Genotype_GVCFs.log"
        GDBI_MEM=$(getMemory "${GDBI_QSUB}")
        # If we used custom intervals for the Haplotype_Caller step, we will need
        # some special processing of the data to get the arrays setup correctly
        #   Note: Check if HC_PARALLELIZE variable is set in case users did not use
        #   sequence_handling to run the Haplotype_Caller step
	    if checkVersion gatk 4.0; then
            gatkVer=4
            # If variable is not unset and hc parallelize is true
            if [ ! -z ${HC_PARALLELIZE+x} ] && [ ${HC_PARALLELIZE} == "true" ]; then
                # GenomicsDBImport() requires reference dict
                checkDict "${REF_GEN}" # Check to make sure our reference genome has a dict file
                if [[ "$?" -ne 0 ]]; then echo "Generating dictionary file for ${REF_GEN}..." >&2; createDict "${REF_GEN}" "${HC_MEM}" "${PICARD_JAR}"; fi # If not, generate one
                analysisType="targeted-HC"
                intvlFile=${HC_CUSTOM_INTERVALS}
                # Use HC_CUSTOM_INTERVALS to determine array limit
                NUM_INTVLS=$(wc -l "${HC_CUSTOM_INTERVALS}" | cut -d ' ' -f 1)
                SINGLE_ARRAY_LIMIT=$[${NUM_INTVLS} - 1]
                # If we also have scaffolds in addition to custom intervals
                # and we are parallelizing across regions, add one to the array limit
                if [[ "${HC_SCAFFOLDS}" != "false" ]]; then
                    SINGLE_ARRAY_LIMIT=${NUM_INTVLS}
                fi
                # If HC_PARRALLELIZE is set to true, PARALLELIZE for Genomics_DB_Import must
                # be set to true for the samples to be handled appropriately
                if [ ${HC_PARALLELIZE} == "true" ]; then
                    PARALLELIZE=true
                fi
                # With gatk4, individual vcf files have to be combined into a DB
                # GenomicsDBImport takes a long time, so omit it if it is already combined.
                # Only check if we are NOT parallelizing across regions (creating a single workspace)
                if [ -d "${OUT_DIR}"/Genotype_GVCFs/combinedDB/gendb_wksp ] && [ $(ls "${OUT_DIR}/Genotype_GVCFs/combinedDB/gendb_wksp" | wc -l) -gt 0 ] && [ "${SINGLE_ARRAY_LIMIT}" -eq 0 ]; then
                    echo "INFO: *** Using pre-existing ${OUT_DIR}/Genotype_GVCFs/combinedDB/gendb_wksp ***" >> "${ERROR}/Genomics_DB_Import.log"
                    echo "There is a pre-existing ${OUT_DIR}/Genotype_GVCFs/combinedDB/gendb_wksp. If this is not what you want to use, delete/rename the existing combinedDB directory and re-run Genomics_DB_Import. If this is correct, proceed to Genotype_GVCFs. Exiting..."
                else
                    echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
                    if [[ "${USE_PBS}" == true ]]; then
                        # If CUSTOM_JOB_ARR is NOT set (only gets set if user uses -t flag)
                        if [[ -z ${CUSTOM_JOB_ARR+x} ]]; then
                            echo "Using default array limit from 0 to ${SINGLE_ARRAY_LIMIT}"
                            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Genomics_DB_Import.sh && GenomicsDBImport ${GVCF_LIST} ${OUT_DIR} ${REF_GEN} ${analysisType} ${intvlFile} ${HC_SCAFFOLDS} ${TMP} ${GDBI_MEM} ${PARALLELIZE}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -l "${GDBI_QSUB}" -q "${GDBI_QUEUE}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_DBImport
                        else
                            # Use job arrays following -t flag
                            echo "Using custom job arrays following -t flag: ${CUSTOM_JOB_ARR}" >&2
                            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Genomics_DB_Import.sh && GenomicsDBImport ${GVCF_LIST} ${OUT_DIR} ${REF_GEN} ${analysisType} ${intvlFile} ${HC_SCAFFOLDS} ${TMP} ${GDBI_MEM} ${PARALLELIZE}" | qsub -t "${CUSTOM_JOB_ARR}" -l "${GDBI_QSUB}" -q "${GDBI_QUEUE}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_DBImport
                        fi
                    fi
                fi
            else
                # Otherwise, if we are using GATK 4.0 and are either NOT providing intervals or
                # providing custom intervals STARTING at the Genomics_DB_Import step, run the following
                # GenomicsDBImport() requires reference dict
                checkDict "${REF_GEN}" # Check to make sure our reference genome has a dict file
                if [[ "$?" -ne 0 ]]; then echo "Generating dictionary file for ${REF_GEN}..." >&2; createDict "${REF_GEN}" "${HC_MEM}" "${PICARD_JAR}"; fi # If not, generate one
                # If we have custom intervals set, check if we are parallelizing across regions
                if [[ "${CUSTOM_INTERVALS}" != "false" ]]; then
                    analysisType="targeted"
                    intvlFile="${CUSTOM_INTERVALS}"
                    # If we have a .intervals/.list file format, check that first position does not start at 0.
                    if [[ "${CUSTOM_INTERVALS}" != *.bed ]]; then
                        # For GATK-style .list or .intervals file in the form <chr>:<start>-<stop>
                        # GATK 4 will throw an error if the first position starts at 0.
                        # Check for this. Fix is to change 0 start position to 1 start position.
                        # This assumes intervals file is sorted
                        temp_start_pos=$(head -n 1 "${CUSTOM_INTERVALS}")
                        if [[ "${temp_start_pos}" == *":0-"* ]]; then
                            echo "GATK-style .list or .intervals file uses 1-based coordinates. Please check your custom intervals file to make sure there are no positions that start at 0. If so, change so the starting position is 1. See docs: https://software.broadinstitute.org/gatk/documentation/article?id=11009. Exiting..." >&2
                            exit 31
                        fi
                    fi
                    # If we are parallelizing across regions, we will have one gendb
                    # workspace for every custom interval
                    if [ "${PARALLELIZE}" == "true" ]; then
                        # Use CUSTOM_INTERVALS to determine array limit
                        NUM_INTVLS=$(wc -l "${CUSTOM_INTERVALS}" | cut -d ' ' -f 1)
                        SINGLE_ARRAY_LIMIT=$[${NUM_INTVLS} - 1]
                        # If we also have scaffolds in addition to custom intervals
                        # and we are parallelizing across regions, add one to the array limit
                        if [[ "${SCAFFOLDS}" != "false" ]]; then
                            SINGLE_ARRAY_LIMIT=${NUM_INTVLS}
                        fi
                    else
                        # We are NOT parallelizing across regions
                        # List of custom intervals is given to GATK, but only a single
                        # gendb workspace is created.
                        SINGLE_ARRAY_LIMIT="0"
                    fi
                else
                    analysisType="WGS"
                    intvlFile="NA"
                    # Use the number of chromosomes to set array limit
                    SINGLE_ARRAY_LIMIT=$[${NUM_CHR} - 1] # Get the maximum number of Torque tasks (# in array - 1)
                    # If we also have scaffolds in addition to the chromosomes, add one to the array limit
                    if [[ "${SCAFFOLDS}" != "false" ]]; then
                        SINGLE_ARRAY_LIMIT=${NUM_CHR}
                    fi
                fi
                # With gatk4, individual vcf files have to be combined into a DB
                # GenomicsDBImport takes a long time, so omit it if it is already combined.
                # Only check if we are NOT parallelizing across regions (creating a single workspace)
                if [ -d "${OUT_DIR}"/Genotype_GVCFs/combinedDB/gendb_wksp ] && [ $(ls "${OUT_DIR}/Genotype_GVCFs/combinedDB/gendb_wksp" | wc -l) -gt 0 ] && [ "${SINGLE_ARRAY_LIMIT}" -eq 0 ]; then
                    echo "INFO: *** Using pre-existing ${OUT_DIR}/Genotype_GVCFs/combinedDB/gendb_wksp ***" >> "${ERROR}/Genomics_DB_Import.log"
                    echo "There is a pre-existing ${OUT_DIR}/Genotype_GVCFs/combinedDB/gendb_wksp. If this is not what you want to use, delete/rename the existing combinedDB directory and re-run Genomics_DB_Import. If this is correct, proceed to Genotype_GVCFs. Exiting..."
                else
                    echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
                    if [[ "${USE_PBS}" == true ]]; then
                        # If we have enough samples for a task array
                        if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]; then
                            # If CUSTOM_JOB_ARR is NOT set (only gets set if user uses -t flag)
                            if [[ -z ${CUSTOM_JOB_ARR+x} ]]
                            then
                                echo "Using default array limit from 0 to ${SINGLE_ARRAY_LIMIT}"
                                echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Genomics_DB_Import.sh && GenomicsDBImport ${GVCF_LIST} ${OUT_DIR} ${REF_GEN} ${analysisType} ${intvlFile} ${SCAFFOLDS} ${TMP} ${GDBI_MEM} ${PARALLELIZE}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -l "${GDBI_QSUB}" -q "${GDBI_QUEUE}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_DBImport
                            else
                                # Use job arrays following -t flag
                                echo "Using custom job arrays following -t flag: ${CUSTOM_JOB_ARR}" >&2
                                echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Genomics_DB_Import.sh && GenomicsDBImport ${GVCF_LIST} ${OUT_DIR} ${REF_GEN} ${analysisType} ${intvlFile} ${SCAFFOLDS} ${TMP} ${GDBI_MEM} ${PARALLELIZE}" | qsub -t "${CUSTOM_JOB_ARR}" -l "${GDBI_QSUB}" -q "${GDBI_QUEUE}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_DBImport
                            fi
                        else # If we are creating a single gendb workspace
                            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Genomics_DB_Import.sh && GenomicsDBImport ${GVCF_LIST} ${OUT_DIR} ${REF_GEN} ${analysisType} ${intvlFile} ${SCAFFOLDS} ${TMP} ${GDBI_MEM} ${PARALLELIZE}" | qsub -t 0 -l "${GDBI_QSUB}" -q "${GDBI_QUEUE}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_DBImport
                        fi
                    else
                        (source "${CONFIG}" && source "${SEQUENCE_HANDLING}/Handlers/Genotype_GVCFs.sh" && GenomicsDBImport "${GVCF_LIST}" "${OUT_DIR}/Genotype_GVCFs/combinedDB" "${REF_GEN}" ${analysisType} "${intvlFile}" "${SCAFFOLDS}" "${TMP}") >> "${ERROR}/Genotype_GVCFs.log" 2>&1
                    fi
                    echo "Making database. Please run Genotype_GVCFs after the database is complete."
                    exit 1
                fi
            fi
	    elif checkVersion gatk 3.8.0 ; then
            gatkVer=3
            echo "Don't need to make database for GATK version 3.8.0 or earlier. Please proceed with Genotype_GVCFs."
	    else
	        echo "Please install GATK 3.8.0 or newer" >&2
	        exit 1
	    fi
        ;;
    8 | Genotype_GVCFs )
        echo "$(basename $0): Genotyping GVCFs..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Genotype_GVCFs.sh
        checkDependencies Genotype_GVCFs_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${GVCF_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
	    # if GATK_JAR is not set in Config, it will check GATK_LOCAL_JAR
	    GATK_JAR=$(checkGATK "${GATK_JAR}") # Check to make sure GATK is installed
        if [[ "${GATK_JAR}" == 1 ]]; then exit 1; fi # If we don't have GATK, exit with error
	    rm -f "${ERROR}/Genotype_GVCFs.log"; touch "${ERROR}/Genotype_GVCFs.log"
	    if checkVersion gatk 4.0 ; then   # check gatk version
	        gatkVer=4
	        # GenomicsDBImport() requires reference dict
	        checkDict "${REF_GEN}" # Check to make sure our reference genome has a dict file
            if [[ "$?" -ne 0 ]]; then echo "Generating dictionary file for ${REF_GEN}..." >&2; createDict "${REF_GEN}" "${HC_MEM}" "${PICARD_JAR}"; fi # If not, generate one
	        if [[ "${CUSTOM_INTERVALS}" != "false" ]]; then
		        analysisType="targeted"
		        intvlFile="${CUSTOM_INTERVALS}"
	        else
		        analysisType="WGS"
		        intvlFile="NA"
	        fi
            # With gatk4, individual vcf files have to be combined into a DB
            # Check if DB exists, if so, proceed with running Genotype_GVCFs
            # GenomicsDBImport takes a long time, so omit it if it is already combined.
            if [ -d "${OUT_DIR}"/Genotype_GVCFs/combinedDB ] && [ $(ls "${OUT_DIR}/Genotype_GVCFs/combinedDB" | wc -l) -gt 0 ]; then
                echo "INFO: *** Using pre-existing ${OUT_DIR}/Genotype_GVCFs/combinedDB ***" >> "${ERROR}/Genotype_GVCFs.log"
            else
                echo "Please run Genomics_DB_Import first to make the database. Exiting..."
                exit 1
            fi
            # Note the gendb:// prefix to the database input directory path is needed for GATK4
            # As of Sep 23, 2019 it seems like we need to be in Genotype_GVCFs for GATK4 to find database
            # CL is unable to get it working with a relative or absolute filepath to the database
            #input_gvcf="gendb://${OUT_DIR}/Genotype_GVCFs/combinedDB"
            input_gvcf="gendb://combinedDB" # Note for GATK 4, this is a placeholder
	    elif checkVersion gatk 3.8.0 ; then
            gatkVer=3
            input_gvcf="${GVCF_LIST}"
	    else
	        echo "Please install GATK 3.8.0 or newer" >&2
	        exit 1
	    fi
        GG_MEM=$(getMemory "${GG_QSUB}") # Figure out memory requirements based on the Qsub settings
        SINGLE_ARRAY_LIMIT=$[${NUM_CHR} - 1] # Get the maximum number of Torque tasks (# in array - 1)
        #set -x # for debugging purposes
        if [[ "${CUSTOM_INTERVALS}" != "false" ]]; then
            analysisType="targeted"
            intvlFile="${CUSTOM_INTERVALS}"
            # If we are parallelizing across regions, we will have one gendb
            # workspace for every custom interval
            if [[ "${PARALLELIZE}" == "true" ]]; then
                # Use CUSTOM_INTERVALS to determine array limit
                NUM_INTVLS=$(wc -l "${CUSTOM_INTERVALS}" | cut -d ' ' -f 1)
                SINGLE_ARRAY_LIMIT=$[${NUM_INTVLS} - 1]
                # If we also have scaffolds in addition to custom intervals
                # and we are parallelizing across regions, add one to the array limit
                if [[ -n "${SCAFFOLDS}" ]]; then
                    SINGLE_ARRAY_LIMIT=${NUM_INTVLS}
                fi
            else
                # We are NOT parallelizing across regions
                # List of custom intervals is given to GATK, but only a single
                # gendb workspace is created.
                # If we have scaffolds, add one to the maximum array index
                SINGLE_ARRAY_LIMIT="0"
            fi
        else
            analysisType="WGS"
            intvlFile="NA"
            # Use the number of chromosomes to set array limit
            SINGLE_ARRAY_LIMIT=$[${NUM_CHR} - 1] # Get the maximum number of Torque tasks (# in array - 1)
            # If we also have scaffolds in addition to the chromosomes, add one to the array limit
            if [[ -n "${SCAFFOLDS}" ]]; then
                SINGLE_ARRAY_LIMIT=${NUM_CHR}
            fi
        fi
	    if [ "${SINGLE_ARRAY_LIMIT}" -lt 0 ]; then
            echo "Please set NUM_CHR > 0. Or if NUM_CHR=0, set CUSTOM_INTERVALS" >&2
            exit 1
        fi
        echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
	    if [[ "$USE_PBS" == true ]]; then
            if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]
            then # If we have enough samples for a task array
                # If CUSTOM_JOB_ARR is NOT set (only gets set if user uses -t flag)
                if [[ -z ${CUSTOM_JOB_ARR+x} ]]
                then
                    echo "Using default array limit from 0 to ${SINGLE_ARRAY_LIMIT}"
		            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Genotype_GVCFs.sh && Genotype_GVCFs ${input_gvcf} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${THETA} ${PLOIDY} ${GG_MEM} ${REF_DICT} ${SINGLE_ARRAY_LIMIT} ${gatkVer} ${intvlFile} ${PARALLELIZE} ${analysisType}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -q "${GG_QUEUE}" -l "${GG_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Genotype_GVCFs
                else
                    # Use job arrays following -t flag
                    echo "Using custom job arrays following -t flag: ${CUSTOM_JOB_ARR}" >&2
                    echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Genotype_GVCFs.sh && Genotype_GVCFs ${input_gvcf} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${THETA} ${PLOIDY} ${GG_MEM} ${REF_DICT} ${SINGLE_ARRAY_LIMIT} ${gatkVer} ${intvlFile} ${PARALLELIZE} ${analysisType}" | qsub -t "${CUSTOM_JOB_ARR}" -q "${GG_QUEUE}" -l "${GG_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Genotype_GVCFs
                fi
            else # If we have only one sample
		        echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Genotype_GVCFs.sh && Genotype_GVCFs ${input_gvcf} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${THETA} ${PLOIDY} ${GG_MEM} ${REF_DICT} ${SINGLE_ARRAY_LIMIT} ${gatkVer} ${intvlFile} ${PARALLELIZE} ${analysisType}" | qsub -t 0 -q "${GG_QUEUE}" -l "${GG_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Genotype_GVCFs
	        fi
	    else # Without PBS
	        source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Genotype_GVCFs.sh && Genotype_GVCFs "${input_gvcf}" "${OUT_DIR}" "${GATK_JAR}" "${REF_GEN}" ${THETA} ${PLOIDY} ${GG_MEM} "${REF_DICT}" ${SINGLE_ARRAY_LIMIT} ${gatkVer} ${intvlFile} >> "${ERROR}/Genotype_GVCFs.log" 2>&1
        fi
        ;;
    9 | Create_HC_Subset )
        echo "$(basename $0): Creating a high-confidence subset of variants..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Create_HC_Subset.sh
        checkDependencies Create_HC_Subset_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${CHS_VCF_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        if [[ -z "${BARLEY}" ]]; then echo "Please specify whether or not the organism is barley in the config file" >&2; exit 1; fi # Make sure barley is filled out
        if [[ -z "${CAPTURE_REGIONS}" ]]; then echo "Please either specify the capture regions bed file or put NA for CAPTURE_REGIONS" >&2; exit 1; fi # Make sure CAPTURE_REGIONS is filled out
	    if [[ "$USE_PBS" == true ]]; then
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Create_HC_Subset.sh && Create_HC_Subset ${CHS_VCF_LIST} ${OUT_DIR} ${CAPTURE_REGIONS} ${BARLEY} ${PROJECT} ${SEQUENCE_HANDLING} ${CHS_QUAL_CUTOFF} ${CHS_GQ_CUTOFF} ${CHS_DP_PER_SAMPLE_CUTOFF} ${CHS_MAX_HET} ${CHS_MAX_BAD}" | qsub -l "${CHS_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Create_HC_Subset
	    else
	        (source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Create_HC_Subset.sh && Create_HC_Subset ${CHS_VCF_LIST} ${OUT_DIR} ${CAPTURE_REGIONS} ${BARLEY} ${PROJECT} ${SEQUENCE_HANDLING} ${CHS_QUAL_CUTOFF} ${CHS_GQ_CUTOFF} ${CHS_DP_PER_SAMPLE_CUTOFF} ${CHS_MAX_HET} ${CHS_MAX_BAD}) > "${ERROR}/Create_HC_Subset.log" 2>&1
	    fi
        ;;
    10 | Variant_Recalibrator )
        echo "$(basename $0): Training model and recalibrating quality scores of variants..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Variant_Recalibrator.sh
        checkDependencies Variant_Recalibrator_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${VR_VCF_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        checkGATK "${GATK_JAR}" # Check to make sure GATK is installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we don't have GATK, exit with error
        if [[ -z "${BARLEY}" ]]; then echo "Please specify whether or not the organism is barley in the config file" >&2; exit 1; fi # Make sure barley is filled out
        checkVCF "${HC_SUBSET}" # Make sure the VCF is formatted properly
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If it's not formatted properly, exit with error
        if ! [[ "${RESOURCE_1}" == "NA" ]]; then checkVCF "${RESOURCE_1}"; fi # Make sure the VCF is formatted properly
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If it's not formatted properly, exit with error
        if ! [[ "${RESOURCE_2}" == "NA" ]]; then checkVCF "${RESOURCE_2}"; fi # Make sure the VCF is formatted properly
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If it's not formatted properly, exit with error
        if ! [[ "${RESOURCE_3}" == "NA" ]]; then checkVCF "${RESOURCE_3}"; fi # Make sure the VCF is formatted properly
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If it's not formatted properly, exit with error
        if ! [[ "${RESOURCE_4}" == "NA" ]]; then checkVCF "${RESOURCE_4}"; fi # Make sure the VCF is formatted properly
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If it's not formatted properly, exit with error
        VR_MEM=$(getMemory "${VR_QSUB}") # Figure out memory requirements based on the Qsub settings
        checkDict "${VR_REF}" # Check to make sure our reference genome has a dict file
        if [[ "$?" -ne 0 ]]; then echo "Generating dictionary file for ${VR_REF}..." >&2; createDict "${VR_REF}" "${VR_MEM}" "${PICARD_JAR}"; fi # If not, generate one
        echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Variant_Recalibrator.sh && Variant_Recalibrator ${VR_VCF_LIST} ${OUT_DIR} ${GATK_JAR} ${VR_REF} ${HC_SUBSET} ${VR_MEM} ${PROJECT} ${RESOURCE_1} ${RESOURCE_2} ${RESOURCE_3} ${RESOURCE_4} ${SEQUENCE_HANDLING} ${PRIOR_1} ${PRIOR_2} ${PRIOR_3} ${PRIOR_4} ${HC_PRIOR} ${BARLEY}" | qsub -q "${VR_QUEUE}" -l "${VR_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Variant_Recalibrator
        ;;
    11 | Variant_Filtering )
        echo "$(basename $0): Filtering variants..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Variant_Filtering.sh
        checkDependencies Variant_Filtering_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        if [[ -z "${BARLEY}" ]]; then echo "Please specify whether or not the organism is barley in the config file" >&2; exit 1; fi # Make sure barley is filled out
        if [[ -z "${VF_CAPTURE_REGIONS}" ]]; then echo "Please either specify the capture regions bed file or put NA for VF_CAPTURE_REGIONS" >&2; exit 1; fi # Make sure CAPTURE_REGIONS is filled out
	    if [[ "$USE_PBS" == true ]]; then
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Variant_Filtering.sh && Variant_Filtering ${VF_VCF} ${OUT_DIR} ${VF_CAPTURE_REGIONS} ${BARLEY} ${PROJECT} ${SEQUENCE_HANDLING} ${MIN_DP} ${MAX_DP} ${MAX_DEV} ${DP_PER_SAMPLE_CUTOFF} ${GQ_CUTOFF} ${MAX_HET} ${MAX_BAD} ${QUAL_CUTOFF}" | qsub -l "${VF_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Variant_Filtering
	    else
	        (source ${CONFIG} && source "${SEQUENCE_HANDLING}/Handlers/Variant_Filtering.sh" && Variant_Filtering "${VF_VCF}" "${OUT_DIR}" "${VF_CAPTURE_REGIONS}" ${BARLEY} ${PROJECT} "${SEQUENCE_HANDLING}" ${MIN_DP} ${MAX_DP} ${MAX_DEV} ${DP_PER_SAMPLE_CUTOFF} ${GQ_CUTOFF} ${MAX_HET} ${MAX_BAD} ${QUAL_CUTOFF}) > "${ERROR}/Variant_Filtering.log" 2>&1
	    fi
        ;;
    12 | Variant_Analysis )
        echo "$(basename $0): Generating summary statistics..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Variant_Analysis.sh
        checkDependencies Variant_Analysis_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        if [[ -z "${VA_VCF}" ]]; then echo "Please specify the VCF file to be analyzed in the config file" >&2; exit 1; fi # Make sure the VCF file is filled out
        if [[ -z "${BARLEY}" ]]; then echo "Please specify whether or not the organism is barley in the config file" >&2; exit 1; fi # Make sure barley is filled out
	    if [[ "$USE_PBS" == true ]]; then
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Variant_Analysis.sh && Variant_Analysis ${VA_VCF} ${OUT_DIR} ${SEQUENCE_HANDLING} ${BARLEY}" | qsub -l "${VA_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Variant_Analysis
	    else
	        (source "${CONFIG}" && source "${SEQUENCE_HANDLING}/Handlers/Variant_Analysis.sh" && Variant_Analysis "${VA_VCF}" "${OUT_DIR}" "${SEQUENCE_HANDLING}" ${BARLEY}) > "${ERROR}/Variant_Analysis.log" 2>&1
	    fi
        ;;
    13 | GBS_Demultiplex )
        echo "$(basename $0): Splitting files based on barcodes..." >&2
        echo "GBS_Demultiplex is not yet functional, exiting..." >&2
        exit 1
        source "${SEQUENCE_HANDLING}"/Handlers/GBS_Demultiplex.sh
        checkDependencies GBS_Demultiplex_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${GBS_SAMPLES}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        barcodeGenerator "${SEQUENCE_HANDLING}" "${KEY_FILE}" "${OUT_DIR}" "${PROJECT}"
        declare -a BARCODE_LIST=($(grep -E ".barcode" "${OUT_DIR}/GBS_Demultiplex/barcodes/${PROJECT}_barcode_list.txt"))
        #   Run GBS_Demultiplexer
        SINGLE_ARRAY_LIMIT=$[${#BARCODE_LIST[@]} - 1] # Get the maximum number of Torque tasks we're doing
        echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
        echo -e "#!/bin/bash\n#PBS -l ${GD_QSUB}\n#PBS -e ${ERROR}\n#PBS -o ${ERROR}\n#PBS -m abe\n#PBS -M ${EMAIL}\nset -e\nset -o pipefail\nsource ${CONFIG}\nsource ${SEQUENCE_HANDLING}/Handlers/GBS_Demultiplex.sh\ndeclare -a BARCODE_LIST=($(grep -E ".barcode" "${OUT_DIR}/GBS_Demultiplex/barcodes/${PROJECT}_barcode_list.txt"))\nSINGLE_ARRAY_LIMIT=\$[\${#BARCODE_LIST[@]} - 1]\nGBS_Demultiplex \${BARCODE_LIST[\${PBS_ARRAYID}]} ${GBS_SAMPLES} ${OUT_DIR} ${LINE_ENDING} ${MISMATCH_TOL} ${PARTIAL} ${FILE_TYPE} ${PROJECT}" > ${PROJECT}_GBS_Demultiplex
        if [[ -z ${CUSTOM_JOB_ARR+x} ]]
        then
            # If CUSTOM_JOB_ARR is NOT set (only gets set if user uses -t flag)
            echo "Using default array limit from 0 to ${SINGLE_ARRAY_LIMIT}"
            qsub -t 0-"${SINGLE_ARRAY_LIMIT}" "${PROJECT}"_GBS_Demultiplex
        else
            # Use job arrays following -t flag
            echo "Using custom job arrays following -t flag: ${CUSTOM_JOB_ARR}" >&2
            qsub -t "${CUSTOM_JOB_ARR}" "${PROJECT}"_GBS_Demultiplex
        fi
        ;;
    14 | Quality_Trimming )
        echo "$(basename $0): Trimming based on quality..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Quality_Trimming.sh
        checkDependencies Quality_Trimming_Dependencies[@] # Check to see if dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${ADAPTED_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        #   Run Quality_Trimming
        echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Quality_Trimming.sh && Quality_Trimming ${ADAPTED_LIST} ${FORWARD_ADAPTED} ${REVERSE_ADAPTED} ${SINGLES_ADAPTED} ${OUT_DIR} ${QT_THRESHOLD} ${QUAL_ENCODING} ${SEQUENCE_HANDLING} ${PROJECT}"| qsub -l "${QT_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Quality_Trimming
        ;;
    15 | Realigner_Target_Creator )
        echo "$(basename $0): Creating targets files for realignment..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Realigner_Target_Creator.sh
        checkDependencies Realigner_Target_Creator_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${RTC_BAM_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        checkBaiIndex "${RTC_BAM_LIST}" # Check to see if our samples are indexed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If they're not indexed, exit out with error
        GATK_JAR=$(checkGATK "${GATK_JAR}") # Check to make sure GATK is installed
        if [[ "${GATK_JAR}" == 1 ]]; then exit 1; fi # If we don't have GATK, exit with error
        if checkVersion gatk 4.0 ; then   # check gatk version
	        echo "Indel realignment functionality is no longer available in GATK 4. Please use GATK 3." >&2
	    elif checkVersion gatk 3.8.0; then
	        gatkVer=3
	    else
	        echo "Please install GATK 3.8.0 or newer" >&2
	        exit 1
	    fi
        RTC_MEM=$(getMemory "${RTC_QSUB}") # Figure out memory requirements based on the Qsub settings
        checkDict "${REF_GEN}" # Check to make sure our reference genome has a dict file
        if [[ "$?" -ne 0 ]]; then echo "Generating dictionary file for ${REF_GEN}..." >&2; createDict "${REF_GEN}" "${RTC_MEM}" "${PICARD_JAR}"; fi # If not, generate one
        declare -a RTC_LIST=($(grep -E ".bam" "${RTC_BAM_LIST}")) # Create an array of the BAM files
        SINGLE_ARRAY_LIMIT=$[${#RTC_LIST[@]} - 1] # Get the maximum number of Torque tasks (# in array - 1)
        echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
        if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]
        then # If we have enough samples for a task array
            # If CUSTOM_JOB_ARR is NOT set (only gets set if user uses -t flag)
            if [[ -z ${CUSTOM_JOB_ARR+x} ]]
            then
                echo "Using default array limit from 0 to ${SINGLE_ARRAY_LIMIT}"
                echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Realigner_Target_Creator.sh && Realigner_Target_Creator ${RTC_BAM_LIST} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${RTC_MEM} ${FIX_QUALITY_SCORES} ${gatkVer}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -l "${RTC_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Realigner_Target_Creator
            else
                # Use job arrays following -t flag
                echo "Using custom job arrays following -t flag: ${CUSTOM_JOB_ARR}" >&2
                echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Realigner_Target_Creator.sh && Realigner_Target_Creator ${RTC_BAM_LIST} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${RTC_MEM} ${FIX_QUALITY_SCORES} ${gatkVer}" | qsub -t "${CUSTOM_JOB_ARR}" -l "${RTC_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Realigner_Target_Creator
            fi
        else # If we have only one sample
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Realigner_Target_Creator.sh && Realigner_Target_Creator ${RTC_BAM_LIST} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${RTC_MEM} ${FIX_QUALITY_SCORES} ${gatkVer}" | qsub -t 0 -l "${RTC_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Realigner_Target_Creator
        fi
        ;;
    16 | Indel_Realigner )
        echo "$(basename $0): Realigning reads around insertions and deletions..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Indel_Realigner.sh
        checkDependencies Indel_Realigner_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${IR_BAM_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        checkBaiIndex "${IR_BAM_LIST}" # Check to see if our samples are indexed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If they're not indexed, exit out with error
        GATK_JAR=$(checkGATK "${GATK_JAR}") # Check to make sure GATK is installed
        if [[ "${GATK_JAR}" == 1 ]]; then exit 1; fi # If we don't have GATK, exit with error
        if checkVersion gatk 4.0 ; then   # check gatk version
	        echo "Indel realignment functionality is no longer available in GATK 4. Please use GATK 3." >&2
	    elif checkVersion gatk 3.8.0; then
	        gatkVer=3
	    else
	        echo "Please install GATK 3.8.0 or newer" >&2
	        exit 1
	    fi
        IR_MEM=$(getMemory "${IR_QSUB}") # Figure out memory requirements based on the Qsub settings
        checkDict "${REF_GEN}" # Check to make sure our reference genome has a dict file
        if [[ "$?" -ne 0 ]]; then echo "Generating dictionary file for ${REF_GEN}..." >&2; createDict "${REF_GEN}" "${IR_MEM}" "${PICARD_JAR}"; fi # If not, generate one
        declare -a IR_LIST=($(grep -E ".bam" "${IR_BAM_LIST}")) # Create an array of the BAM files
        SINGLE_ARRAY_LIMIT=$[${#IR_LIST[@]} - 1] # Get the maximum number of Torque tasks (# in array - 1)
        echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
        if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]
        then # If we have enough samples for a task array
            # If CUSTOM_JOB_ARR is NOT set (only gets set if user uses -t flag)
            if [[ -z ${CUSTOM_JOB_ARR+x} ]]
            then
                echo "Using default array limit from 0 to ${SINGLE_ARRAY_LIMIT}"
                echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Indel_Realigner.sh && Indel_Realigner ${IR_BAM_LIST} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${IR_MEM} ${IR_TARGETS} ${LOD_THRESHOLD} ${ENTROPY_THRESHOLD} ${FIX_QUALITY_SCORES} ${gatkVer}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -l "${IR_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Indel_Realigner
            else
                # Use job arrays following -t flag
                echo "Using custom job arrays following -t flag: ${CUSTOM_JOB_ARR}" >&2
                echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Indel_Realigner.sh && Indel_Realigner ${IR_BAM_LIST} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${IR_MEM} ${IR_TARGETS} ${LOD_THRESHOLD} ${ENTROPY_THRESHOLD} ${FIX_QUALITY_SCORES} ${gatkVer}" | qsub -t "${CUSTOM_JOB_ARR}" -l "${IR_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Indel_Realigner
            fi
        else # If we have only one sample
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/Indel_Realigner.sh && Indel_Realigner ${IR_BAM_LIST} ${OUT_DIR} ${GATK_JAR} ${REF_GEN} ${IR_MEM} ${IR_TARGETS} ${LOD_THRESHOLD} ${ENTROPY_THRESHOLD} ${FIX_QUALITY_SCORES} ${gatkVer}" | qsub -t 0 -l "${IR_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_Indel_Realigner
        fi
        ;;
    17 | Freebayes_Variant_Calling )
        echo "$(basename $0): Variant calling using Freebayes..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/Freebayes_VC.sh
        checkDependencies Freebayes_VC_Dependencies[@]
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        ;;
    1NP | NP_Quality_Assessment )
        echo "$(basename $0): Assessing quality..." >&2
        source "${SEQUENCE_HANDLING}"/Handlers/NP_Quality_Assessment.sh
        checkDependencies NP_Quality_Assessment_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        #if [[ ! -d "${FAST5_DIRECTORY}" ]]; then echo "Error: Failed to find FAST5 directory at "${FAST5_DIRECTORY}", exiting..." >&2; exit 1; fi # If the sample directory doesn't exist, exit
        checkSamples "${QA_SAMPLE_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        declare -a NP_QA_ARRAY=($(grep -E ".fastq|.fastq.gz" "${QA_SAMPLE_LIST}")) # Create an array of the files
        SINGLE_ARRAY_LIMIT=$[${#NP_QA_ARRAY[@]} - 1] # Get the maximum number of Torque tasks (# in array - 1)
        echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
        if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]
        then # If we have enough samples for a task array
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/NP_Quality_Assessment.sh && NP_Quality_Assessment ${QA_SAMPLE_LIST} ${OUT_DIR} ${PROJECT}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -l "${QA_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_NP_Quality_Assessment
        else # If we have only one sample
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/NP_Quality_Assessment.sh && NP_Quality_Assessment ${QA_SAMPLE_LIST} ${OUT_DIR} ${PROJECT}" | qsub -t 0 -l "${QA_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_NP_Quality_Assessment
        fi
        ;;
    2NP | NP_Adapter_Trimming )
        echo "$(basename $0): Trimming adapters..." >&2
        source "${SEQUENCE_HANDLING}/Handlers/NP_Adapter_Trimming.sh"
        checkDependencies NP_Adapter_Trimming_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${AT_SAMPLE_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        declare -a AT_ARRAY=($(grep -E ".fastq|.fastq.gz|.fasta|.fasta.gz|.fa|.fq|.fa.gz|.fq.gz" "${AT_SAMPLE_LIST}")) # Create an array of the files
        SINGLE_ARRAY_LIMIT=$[${#AT_ARRAY[@]} - 1] # Get the maximum number of Torque tasks (# in array - 1)
        echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
        if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]
        then # If we have enough samples for a task array
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/NP_Adapter_Trimming.sh && NP_Adapter_Trimming ${AT_ARRAY} ${OUT_DIR} ${PROJECT}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -l "${AT_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_NP_Adapter_Trimming
        else # If we have only one sample
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/NP_Adapter_Trimming.sh && NP_Adapter_Trimming ${AT_ARRAY} ${OUT_DIR} ${PROJECT}" | qsub -t 0 -l "${AT_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_NP_Adapter_Trimming
        fi
        ;;
    3NP | NP_Read_Mapping )
        echo "$(basename $0): Mapping reads..." >&2
        source "${SEQUENCE_HANDLING}/Handlers/NP_Read_Mapping.sh"
        checkDependencies NP_Read_Mapping_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${RM_SAMPLE_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        declare -a RM_ARRAY=($(grep -E ".fastq|.fastq.gz|.fasta|.fasta.gz|.fa|.fq|.fa.gz|.fq.gz" "${RM_SAMPLE_LIST}")) # Create an array of the files
        SINGLE_ARRAY_LIMIT=$[${#RM_ARRAY[@]} - 1] # Get the maximum number of Torque tasks (# in array - 1)
        echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
        if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]
        then # If we have enough samples for a task array
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/NP_Read_Mapping.sh && NP_Read_Mapping ${RM_SAMPLE_LIST} ${OUT_DIR} ${OUTPUT_FORMAT} ${REF_GEN} ${REFERENCE_MINIMAP_INDEX} ${BANDWIDTH} ${MATCHING_SCORE} ${MISMATCH_PENALTY} ${GAP_OPEN_PENALTY} ${GAP_EXTENSION_PENALTY} ${Z_DROP_SCORE} ${MINIMAL_PEAK_DP_SCORE} ${THREADS}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -l "${RM_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_NP_Read_Mapping
        else # If we have only one sample
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/NP_Read_Mapping.sh && NP_Read_Mapping ${RM_SAMPLE_LIST} ${OUT_DIR} ${OUTPUT_FORMAT} ${REF_GEN} ${REFERENCE_MINIMAP_INDEX} ${BANDWIDTH} ${MATCHING_SCORE} ${MISMATCH_PENALTY} ${GAP_OPEN_PENALTY} ${GAP_EXTENSION_PENALTY} ${Z_DROP_SCORE} ${MINIMAL_PEAK_DP_SCORE} ${THREADS}" | qsub -t 0 -l "${RM_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_NP_Read_Mapping
        fi
        ;;
    4NP | NP_SAM_Processing )
        echo "$(basename $0): Converting SAM files into BAM files..." >&2
        source "${SEQUENCE_HANDLING}/Handlers/NP_SAM_Processing.sh"
        checkDependencies NP_SAM_Processing_Dependencies[@] # Check to see if the dependencies are installed
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a dependency, exit out with error
        checkSamples "${SP_SAMPLE_LIST}" # Check to see if samples and sample list exists
        if [[ "$?" -ne 0 ]]; then exit 1; fi # If we're missing a sample or our list, exit out with error
        checkVersion 'samtools' '1.3' # Check SAMtools version 1.3 or higher
        if [[ "$?" -ne 0 ]]; then echo "Please use SAMtools version 1.3 or higher" >&2; exit 1; fi
        #   Create the header for the mapping stats summary file
        mkdir -p "${OUT_DIR}/SAM_Processing/Statistics"
        echo -e "Sample name\tTotal reads\tPercent mapped" > "${OUT_DIR}/SAM_Processing/Statistics/${PROJECT}_mapping_summary.tsv"
        #   Run SAM_Processing using a task array
        declare -a SAM_LIST=($(grep -E ".sam" "${SP_SAMPLE_LIST}"))
        SINGLE_ARRAY_LIMIT=$[${#SAM_LIST[@]} - 1] # Get the maximum number of Torque tasks we're doing for SAM samples
        echo "Max array index is ${SINGLE_ARRAY_LIMIT}...">&2
        if [[ "${SINGLE_ARRAY_LIMIT}" -ne 0 ]]
        then # If we have enough samples for a task array
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/NP_SAM_Processing.sh && NP_SAM_Processing ${SP_SAMPLE_LIST} ${OUT_DIR} ${REF_GEN} ${PROJECT}" | qsub -t 0-"${SINGLE_ARRAY_LIMIT}" -l "${SP_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_NP_SAM_Processing
        else # If we have only one sample
            echo "source ${CONFIG} && source ${SEQUENCE_HANDLING}/Handlers/NP_SAM_Processing.sh && NP_SAM_Processing ${SP_SAMPLE_LIST} ${OUT_DIR} ${REF_GEN} ${PROJECT}" | qsub -t 0 -l "${SP_QSUB}" -e "${ERROR}" -o "${ERROR}" -m abe -M "${EMAIL}" -N "${PROJECT}"_NP_SAM_Processing
        fi
        ;;
    * )
        Usage
        ;;
esac
